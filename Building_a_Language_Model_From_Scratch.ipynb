{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "class SimpleTokenizer:\n",
        "    \"\"\"\n",
        "    A simple character-level tokenizer.\n",
        "    \"\"\"\n",
        "    def __init__(self, corpus):\n",
        "        self.vocab = sorted(list(set(corpus)))\n",
        "        self.vocab_size = len(self.vocab)\n",
        "        self.char_to_idx = {ch: i for i, ch in enumerate(self.vocab)}\n",
        "        self.idx_to_char = {i: ch for i, ch in enumerate(self.vocab)}\n",
        "\n",
        "        self.padding_idx = self.vocab_size\n",
        "        self.vocab_size += 1\n",
        "\n",
        "\n",
        "    def encode(self, text):\n",
        "        \"\"\"Converts a string of text into a list of integers.\"\"\"\n",
        "        return [self.char_to_idx[ch] for ch in text]\n",
        "\n",
        "    def decode(self, indices):\n",
        "        \"\"\"Converts a list of integers back into a string of text.\"\"\"\n",
        "\n",
        "        return \"\".join([self.idx_to_char[i] for i in indices if i != self.padding_idx])\n",
        "\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    A simple self-attention mechanism.\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_size, heads):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.heads = heads\n",
        "        self.head_dim = embed_size // heads\n",
        "\n",
        "        assert (\n",
        "            self.head_dim * heads == embed_size\n",
        "        ), \"Embedding size needs to be divisible by heads\"\n",
        "\n",
        "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
        "\n",
        "    def forward(self, values, keys, query, mask):\n",
        "        N = query.shape[0]\n",
        "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
        "\n",
        "\n",
        "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
        "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
        "        queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
        "\n",
        "        values = self.values(values)\n",
        "        keys = self.keys(keys)\n",
        "        queries = self.queries(queries)\n",
        "\n",
        "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
        "\n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
        "\n",
        "        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)\n",
        "\n",
        "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
        "            N, query_len, self.heads * self.head_dim\n",
        "        )\n",
        "\n",
        "        out = self.fc_out(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    A single block of the Transformer architecture.\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.attention = SelfAttention(embed_size, heads)\n",
        "        self.norm1 = nn.LayerNorm(embed_size)\n",
        "        self.norm2 = nn.LayerNorm(embed_size)\n",
        "\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(forward_expansion * embed_size, embed_size),\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, value, key, query, mask):\n",
        "        attention = self.attention(value, key, query, mask)\n",
        "        x = self.dropout(self.norm1(attention + query))\n",
        "        forward = self.feed_forward(x)\n",
        "        out = self.dropout(self.norm2(forward + x))\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    \"\"\"\n",
        "    The full Transformer architecture.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size,\n",
        "        embed_size,\n",
        "        num_layers,\n",
        "        heads,\n",
        "        device,\n",
        "        forward_expansion,\n",
        "        dropout,\n",
        "        max_length,\n",
        "        padding_idx\n",
        "    ):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.device = device\n",
        "        self.word_embedding = nn.Embedding(vocab_size, embed_size, padding_idx=padding_idx)\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.layers = nn.ModuleList(\n",
        "            [\n",
        "                TransformerBlock(\n",
        "                    embed_size,\n",
        "                    heads,\n",
        "                    dropout=dropout,\n",
        "                    forward_expansion=forward_expansion,\n",
        "                )\n",
        "                for _ in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.fc_out = nn.Linear(embed_size, vocab_size - 1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        N, seq_length = x.shape\n",
        "        word_embeddings = self.word_embedding(x)\n",
        "\n",
        "\n",
        "        position = torch.arange(0, seq_length, dtype=torch.float).unsqueeze(1).to(self.device)\n",
        "        div_term = torch.exp(torch.arange(0, self.embed_size, 2).float() * (-math.log(10000.0) / self.embed_size)).to(self.device)\n",
        "\n",
        "        position_encodings = torch.zeros(seq_length, self.embed_size).to(self.device)\n",
        "        position_encodings[:, 0::2] = torch.sin(position * div_term)\n",
        "        position_encodings[:, 1::2] = torch.cos(position * div_term)\n",
        "        position_encodings = position_encodings.unsqueeze(0).expand(N, seq_length, self.embed_size)\n",
        "\n",
        "        out = self.dropout(word_embeddings + position_encodings)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            out = layer(out, out, out, mask)\n",
        "\n",
        "\n",
        "        out = self.fc_out(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "class MLMDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A dataset for masked language modeling.\n",
        "    \"\"\"\n",
        "    def __init__(self, corpus, tokenizer, max_length):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.input_sequences = []\n",
        "        self.target_sequences = []\n",
        "\n",
        "        encoded_corpus = self.tokenizer.encode(corpus)\n",
        "\n",
        "\n",
        "        for i in range(len(encoded_corpus) - max_length):\n",
        "\n",
        "            chunk = encoded_corpus[i : i + max_length + 1]\n",
        "\n",
        "\n",
        "            input_seq = torch.tensor(chunk[:-1], dtype=torch.long)\n",
        "            target_seq = torch.tensor(chunk[1:], dtype=torch.long)\n",
        "\n",
        "            self.input_sequences.append(input_seq)\n",
        "            self.target_sequences.append(target_seq)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_sequences[idx], self.target_sequences[idx]\n",
        "\n",
        "def pretrain(model, dataloader, optimizer, criterion, epochs):\n",
        "    \"\"\"\n",
        "    The pre-training loop.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        for input_seq, target_seq in dataloader:\n",
        "            input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
        "\n",
        "\n",
        "            mask = torch.tril(torch.ones(input_seq.shape[1], input_seq.shape[1])).to(device)\n",
        "\n",
        "\n",
        "            output = model(input_seq, mask)\n",
        "\n",
        "\n",
        "            loss = criterion(output.view(-1, output.shape[-1]), target_seq.view(-1))\n",
        "\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(f\"Pre-training Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "class SentimentDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A dataset for sentiment analysis.\n",
        "    \"\"\"\n",
        "    def __init__(self, data, tokenizer, max_length):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text, label = self.data[idx]\n",
        "        encoded_text = self.tokenizer.encode(text)\n",
        "\n",
        "\n",
        "        if len(encoded_text) < self.max_length:\n",
        "            padded_text = encoded_text + [self.tokenizer.padding_idx] * (self.max_length - len(encoded_text))\n",
        "        else:\n",
        "            padded_text = encoded_text[:self.max_length]\n",
        "\n",
        "        return torch.tensor(padded_text, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "\n",
        "def finetune(model, dataloader, optimizer, criterion, epochs):\n",
        "    \"\"\"\n",
        "    The fine-tuning loop.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        for text, label in dataloader:\n",
        "            text, label = text.to(device), label.to(device)\n",
        "\n",
        "\n",
        "            output = model(text, mask=None)\n",
        "\n",
        "\n",
        "            prediction = output[:, -1, :]\n",
        "\n",
        "\n",
        "            loss = criterion(prediction, label)\n",
        "\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(f\"Fine-tuning Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "    print(\"--- Starting Pre-training ---\")\n",
        "\n",
        "\n",
        "    pretrain_corpus = \"This is a simple corpus for pre-training our model. It contains some text to learn from.\"\n",
        "\n",
        "\n",
        "    finetune_data = [\n",
        "        (\"this is a great movie\", 1),\n",
        "        (\"i hated this film\", 0),\n",
        "        (\"the acting was terrible\", 0),\n",
        "        (\"a true masterpiece\", 1),\n",
        "    ]\n",
        "\n",
        "    inference_text = \"this movie was amazing\"\n",
        "\n",
        "\n",
        "\n",
        "    combined_corpus = pretrain_corpus + \"\".join([text for text, label in finetune_data]) + inference_text\n",
        "\n",
        "\n",
        "\n",
        "    tokenizer = SimpleTokenizer(combined_corpus)\n",
        "\n",
        "\n",
        "\n",
        "    pretrain_dataset = MLMDataset(pretrain_corpus, tokenizer, max_length=20)\n",
        "    pretrain_dataloader = DataLoader(pretrain_dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "\n",
        "    model = Transformer(\n",
        "        vocab_size=tokenizer.vocab_size,\n",
        "        embed_size=256,\n",
        "        num_layers=2,\n",
        "        heads=4,\n",
        "        device=device,\n",
        "        forward_expansion=4,\n",
        "        dropout=0.1,\n",
        "        max_length=20,\n",
        "        padding_idx=tokenizer.padding_idx\n",
        "    ).to(device)\n",
        "\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.padding_idx)\n",
        "\n",
        "\n",
        "    pretrain(model, pretrain_dataloader, optimizer, criterion, epochs=10)\n",
        "\n",
        "    print(\"\\n--- Pre-training Finished ---\")\n",
        "\n",
        "\n",
        "    print(\"\\n--- Starting Fine-tuning ---\")\n",
        "\n",
        "\n",
        "\n",
        "    finetune_dataset = SentimentDataset(finetune_data, tokenizer, max_length=20)\n",
        "    finetune_dataloader = DataLoader(finetune_dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "\n",
        "    model.fc_out = nn.Linear(model.embed_size, 2).to(device)\n",
        "\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "    finetune(model, finetune_dataloader, optimizer, criterion, epochs=10)\n",
        "\n",
        "    print(\"\\n--- Fine-tuning Finished ---\")\n",
        "\n",
        "\n",
        "    print(\"\\n--- Running Inference ---\")\n",
        "\n",
        "    model.eval()\n",
        "    test_text = \"this movie was amazing\"\n",
        "    encoded_text = tokenizer.encode(test_text)\n",
        "\n",
        "    padded_text = encoded_text + [tokenizer.padding_idx] * (20 - len(encoded_text))\n",
        "    input_tensor = torch.tensor([padded_text], dtype=torch.long).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(input_tensor, mask=None)\n",
        "        prediction = torch.argmax(output[:, -1, :], dim=1).item()\n",
        "\n",
        "    sentiment = \"Positive\" if prediction == 1 else \"Negative\"\n",
        "    print(f\"Text: '{test_text}'\")\n",
        "    print(f\"Predicted sentiment: {sentiment}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Pre-training ---\n",
            "Pre-training Epoch 1/10, Loss: 1.2131\n",
            "Pre-training Epoch 2/10, Loss: 0.7460\n",
            "Pre-training Epoch 3/10, Loss: 0.6884\n",
            "Pre-training Epoch 4/10, Loss: 0.3699\n",
            "Pre-training Epoch 5/10, Loss: 0.3553\n",
            "Pre-training Epoch 6/10, Loss: 0.3578\n",
            "Pre-training Epoch 7/10, Loss: 0.2421\n",
            "Pre-training Epoch 8/10, Loss: 0.3084\n",
            "Pre-training Epoch 9/10, Loss: 0.2085\n",
            "Pre-training Epoch 10/10, Loss: 0.3429\n",
            "\n",
            "--- Pre-training Finished ---\n",
            "\n",
            "--- Starting Fine-tuning ---\n",
            "Fine-tuning Epoch 1/10, Loss: 0.8570\n",
            "Fine-tuning Epoch 2/10, Loss: 0.8053\n",
            "Fine-tuning Epoch 3/10, Loss: 0.5933\n",
            "Fine-tuning Epoch 4/10, Loss: 0.5676\n",
            "Fine-tuning Epoch 5/10, Loss: 0.3893\n",
            "Fine-tuning Epoch 6/10, Loss: 0.4958\n",
            "Fine-tuning Epoch 7/10, Loss: 0.4457\n",
            "Fine-tuning Epoch 8/10, Loss: 0.5735\n",
            "Fine-tuning Epoch 9/10, Loss: 0.5418\n",
            "Fine-tuning Epoch 10/10, Loss: 0.5020\n",
            "\n",
            "--- Fine-tuning Finished ---\n",
            "\n",
            "--- Running Inference ---\n",
            "Text: 'this movie was amazing'\n",
            "Predicted sentiment: Positive\n"
          ]
        }
      ],
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OYfRraPKt-K",
        "outputId": "79ca1a98-4051-4922-bd3e-69b273ae9061"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}